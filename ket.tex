% to start editing, read the content template.tex in here and start
% editing.
\section{KET: Elementary Particle Physics}

\sectauthor{Alexander Schmidt, RWTH Aachen}

\subsection{Existing Federated Infrastructures}
Over the last decades the field of experimental high-energy particle
physics has experienced a dramatic inflation of both the amount of
data and the required resources to process them. In particular, the
experiments at the CERN Large Hadron Collider (LHC) have played a
pioneering role. The Worldwide LHC Computing Grid (WLCG) project has
been founded to handle and process the data produced by the LHC
experiments. Still today  the WLCG is the world's largest computing
grid. It consists of around 170 computing centers in more than 40
countries.  It is supported by many associated national and
international grids across the world, such as European Grid
Infrastructure (EGI) and Open Science Grid (OSG), based in the US, as well as many other
regional grids\todo{Are these two federated?  If so, what are the
experiences?  If not, why not?\\CW: OSG and EGI structures are transparentlty federated in WLCG.}.  The WLCG consists of four layers, or ``tiers''; 0, 1, 2
and 3. Each tier provides a specific set of services. The Tier-0 is at
CERN and is responsible for the safe-keeping of the raw data (first
copy). Thirteen Tier-1\todo{That should be 1, right?\\CW: yes, fixed} centers are responsible for a proportional
share of raw and reconstructed data, large-scale reprocessing and
safe-keeping of corresponding output, distribution of data to Tier-2s.\todo{The roles of T1 and T2 have become more and more equal.} The Tier-2s are typically universities and other scientific
institutes, which can store sufficient data and provide adequate
computing power for specific analysis tasks. They handle analysis
requirements and proportional share of simulated event production and
reconstruction.
\todo{Orig: Add descriptions of other federated infrastructures in HEP?
Add descriptions of KHUK?}


\subsection{Technologies Employed}
Currently the WLCG defines four component layers,  networking, hardware, middleware and
physics analysis software. The most important middleware stacks used
in the WLCG are from the European Middleware Initiative, which
combines several middleware providers (ARC, gLite, UNICORE\todo{UNICORE is of no relevance for WLCG, basically no gLite component is used nawadays.} and
dCache); the Globus Toolkit developed by the Globus Alliance; and the
Virtual Data Toolkit.\todo{some of those: gloss?}



\subsection{Current Issues, future challenges}
In the realm of LHC computing, the data volume and complexity will
increase dramatically with the upcoming high-luminosity phase of the
collider. Compared to 2016, an increase of computing resources by a
factor of 60 will be needed \todo{The factor 60 would be required, if the Run2 computing model is extrapolated to Run4. New models forecast a much smaller gap}. However, assuming a flat budget, the
increase through improved technologies is expected to be only 6 to 10. Also the
experiments at FAIR\todo{Is that the FAIR-FAIR, and if so, what part of
it would be an experiment?\\CW: This is the FAIR facility at GSI.} and and new large scale projects in astro-particle
physics will have similar requirements. 

To meet these challenges, new technologies for the access and use of
existing and future computing resources will be needed. The new
technologies evolve around the concept of a ``science cloud'' with the
approach to implement complete infrastructures as services (IaaS,
Infrastructure as a Service). This way, not only individual
applications, but also complex infrastructures consisting of a
multitude of services can be provided virtually\todo{How would that help
with the large data volumes?\\CW: Not al all! To safe data volume one needs smaller data tiers and smarter replica management.}. The resources to be
used are characterised through a great heterogeneity (see details in section
\ref{lab:ketfed}). 

\subsection{Infrastructures to be Federated \label{lab:ketfed} }
The resources to be used consist of installations with dedicated CPU
and storage systems at large computing centers, but also analysis
farms at smaller institutes as well as only temporarily available
resources at partner instutions and HPC centers (including
supercomputing resources of the Gauss-alliance). But also commercial
cloud systems and cloud access at universities and research facilities
need to be included. The use of GPU clusters is an attractive option
that is investigated as well. For the efficient and robust operation
of experiment specific software at such a variety of heterogeneous
resources a high level of abstraction of work flows is required and
the development of dedicated software tools is needed. This is needed
to allow the use of a multitude of resources spread across multiple
locations without the secial expertise of the local user
community. The use of more federated and commonly used resources will
also allow to use them in a more flexible way to increase the
utilization (occupancy?) of resources. 

\subsection{First approaches for solutions, future technologies}
Since 2017 the German ErUM community (Erforschung von Universum und
Materie) has started to work on common solutions to the challenges
listed above. The pilot project IDT-UM (2017-2021) is being continued
with the FIDIUM project (2021-2024), in which the intelligent
techniques for abstraction, data management and accounting will be
developed.\todo{orig: Add more details about technologies here?}
